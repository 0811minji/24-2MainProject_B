{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3j8TIOzF_jT"
      },
      "source": [
        "# NonStaticCNN_모델 만들기.ipynb\n",
        "\n",
        "**1.FastText를 사용한 초기 임베딩 레이어**\n",
        "- 임베딩 레이어; 단어간의 의미 유사도를 보여주는 행렬\n",
        "- 기존에는 초기 임베딩 레이어는 0으로 하고, 영화리뷰 90%, 맛집리뷰 10%의 비율로 임베딩 레이어가 생성되므로 영화도메인, 맛집도메인에서 차이가 클 때 긍부정 분류가 잘 안될 수 있음.\n",
        "- FastText에서 사전 학습된 초기 임베딩 레이어로 시작.\n",
        "  - FastText는 서브워드 기반으로 단어 OOV 문제를 완화하며, 비격식적 말투에서도 기본적인 성능을 제공.\n",
        "  -[FastText 다운로드링크](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md) 아래로 내려가면 한국어에서 `cc.ko.300.vec.gz` 파일을 받으면 된다.\n",
        "\n",
        "\n",
        "**2. 문장 분리**\n",
        "- kkoma(꼬꼬마)를 사용하면 문장부호 없이도 문장을 분리할 수 있다고 함.\n",
        "- 하나의 리뷰에 여러개의 감정이 섞여있을 수 있어서 kkoma를 사용한 문장분리 추가.\n",
        "- FastText는 서브워드 기반이므로 문법 교정의 필요성이 낮아지므로 문법 교정은 생략\n",
        "\n",
        "\n",
        "**3. 초기 학습 데이터**\n",
        "- AIhub에서 제공되는 한국어 단발성 일반 담화 데이터로 대체함. [링크](https://aihub.or.kr/aihubdata/data/view.do?dataSetSn=270)\n",
        "- 도메인에 상관없이 모든 상황에서 기초로 적용할 수 있는 담화 데이터임.\n",
        "\n",
        "\n",
        "### Non Static CNN\n",
        "우리 프로젝트에서 적합한 이유\n",
        "- 로컬 특징을 잘 추출해서 짧은 텍스트에 적합함: CNN은 다양한 크기의 필터를 사용해 텍스트 내의 로컬 패턴(2~4단어 수준의 n-그램)을 잘 학습.\n",
        "- 임베딩을 업데이트할 수 있음: 사전 학습된 임베딩에서 시작해서 특정 도메인에 특화된 임베딩으로 수정이 가능함. 맛집 특화된 임베딩이 없는 현 상황에서 최선의 선택지일 것 같음.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUZn7yiyF_jU"
      },
      "outputs": [],
      "source": [
        "#'세션 다시 시작'이라는 안내메시지가 나오는 경우, \"취소\"하면 됨. (설치는 문제없이 되는듯??)\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
        "!pip install git+https://github.com/ssut/py-hanspell.git\n",
        "!pip install konlpy\n",
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaPLl4yJF_jU",
        "outputId": "afa37bb3-2575-4bfc-832d-3eeac4113acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5VKi_plF_jU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import pickle\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from konlpy.tag import Kkma\n",
        "from pykospacing import Spacing\n",
        "from hanspell import spell_checker\n",
        "\n",
        "import os\n",
        "os.environ['JAVA_TOOL_OPTIONS'] = \"-Xmx4g -Xms512m\" #텍스트 전처리에서 과부하 문제가 있어서 방지용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnr80WX3F_jU"
      },
      "outputs": [],
      "source": [
        "# 1. FastText 임베딩 로드\n",
        "def load_vectors(fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())  # 첫 줄: 단어 수와 차원 수 읽기\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')  # 공백 기준으로 단어와 벡터 분리\n",
        "        word = tokens[0]  # 단어\n",
        "        vector = list(map(float, tokens[1:]))  # 벡터값 (실수로 변환)\n",
        "        data[word] = vector  # 딕셔너리에 저장\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b-a4KQSF_jU"
      },
      "outputs": [],
      "source": [
        "# 2. 임베딩 행렬 생성\n",
        "def create_embedding_matrix(tokenizer, embedding_dict, vocab_size, embedding_dim):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i >= vocab_size:\n",
        "            continue\n",
        "        vector = embedding_dict.get(word)\n",
        "        if vector is not None:\n",
        "            embedding_matrix[i] = vector\n",
        "    print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of2SlNNSF_jU"
      },
      "outputs": [],
      "source": [
        "# 3. 비정태적 CNN 모델 정의 (참고논문과 같은 모델으로 정의함)\n",
        "def build_nonstatic_cnn(vocab_size, embedding_dim, embedding_matrix, max_len):\n",
        "    # 1. 입력 계층 정의\n",
        "    input_layer = Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "    # 2. 임베딩 레이어: 비정태적 (사전 학습된 임베딩에서 시작, 학습 가능)\n",
        "    embedding_layer = Embedding(input_dim =vocab_size,\n",
        "                                 output_dim=embedding_dim,\n",
        "                                 weights=[embedding_matrix],\n",
        "                                 trainable=True)(input_layer) #임베딩 레이어를 학습 가능한 상태로 설정\n",
        "\n",
        "    # 3. 합성곱 계층: 다양한 필터 크기 사용 (2, 3, 4, 5)\n",
        "    conv_2 = Conv1D(filters=50, kernel_size=2, activation='relu', padding='same')(embedding_layer)\n",
        "    conv_3 = Conv1D(filters=50, kernel_size=3, activation='relu', padding='same')(embedding_layer)\n",
        "    conv_4 = Conv1D(filters=50, kernel_size=4, activation='relu', padding='same')(embedding_layer)\n",
        "\n",
        "    # 4. 맥스풀링 계층: Global MaxPooling 사용\n",
        "    pool_2 = GlobalMaxPooling1D()(conv_2)\n",
        "    pool_3 = GlobalMaxPooling1D()(conv_3)\n",
        "    pool_4 = GlobalMaxPooling1D()(conv_4)\n",
        "    # Global MaxPooling은 가장 중요한 특정 패턴만 남기고 결과물의 크기를 축소하여 1x50으로 통일함.\n",
        "    # 예를 들어, 리뷰가 blah blah 정말 맛있어요 blah blah라면, 다른 패턴보다 '정말 맛있어요'에 집중\n",
        "\n",
        "    # 5. 출력 연결: 여러 맥스풀링 결과를 Concatenate\n",
        "    concatenated = Concatenate()([pool_2, pool_3, pool_4])\n",
        "\n",
        "    # 6. Fully Connected Layer 1: 은닉층 (50 units)\n",
        "    dense_1 = Dense(50, activation='relu')(concatenated)\n",
        "    dropout_1 = Dropout(0.5)(dense_1)\n",
        "\n",
        "    # 7. Fully Connected Layer 2: 출력층 (이진 분류 - Sigmoid)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dropout_1)\n",
        "\n",
        "    # 8. 모델 생성\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBMVj4ciF_jV"
      },
      "outputs": [],
      "source": [
        "# 4. 데이터 전처리\n",
        "\n",
        "# 꼬꼬마 문장 분리 객체 생성\n",
        "kkma = Kkma()\n",
        "\n",
        "# 1. 전처리 함수: 특수문자 제거, 공백 처리, 맞춤법 교정\n",
        "def preprocess_text(data):\n",
        "    #데이터프레임 형식이 아닌 경우 오류남\n",
        "    if isinstance(data, (list, np.ndarray)):\n",
        "        data = pd.DataFrame(data, columns=['reviews'])\n",
        "    elif not isinstance(data, pd.DataFrame):\n",
        "        raise ValueError(\"입력 데이터는 DataFrame, list, 또는 ndarray 형태여야 합니다.\")\n",
        "\n",
        "    # 한글, 숫자, 공백, 언더스코어 외의 모든 문자 제거 (\\n 포함)\n",
        "    data['reviews'] = data['reviews'].str.replace(\"[^\\w\\sㄱ-ㅎㅏ-ㅣ가-힣]\", \" \", regex=True)\n",
        "\n",
        "    # 맞춤법 및 띄어쓰기 교정\n",
        "    #data['reviews'] = data['reviews'].apply(lambda x: spacing(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # 공백만 남은 경우 NaN 처리\n",
        "    data['reviews'].replace('', np.nan, inplace=True)\n",
        "\n",
        "    #전처리 후 남은 글자가 2글자 미만이라면 삭제함\n",
        "    data['reviews'] = data['reviews'].apply(lambda x: np.nan if isinstance(x,str) and len(x.strip()) <= 5 else x)\n",
        "\n",
        "    # NaN 제거\n",
        "    data.dropna(subset=['reviews'], inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 2. 문장 분리 함수: 꼬꼬마를 활용해 문장 단위로 분리\n",
        "def split_sentences(data):\n",
        "    if isinstance(data, list) or isinstance(data, np.ndarray):\n",
        "        data = pd.DataFrame(data, columns=['reviews'])\n",
        "    data['reviews'] = data['reviews'].apply(lambda x: kkma.sentences(x) if isinstance(x, str) else [])\n",
        "    data = data.explode('reviews').reset_index(drop=True)\n",
        "    return data\n",
        "\n",
        "# 3. 최종 데이터 전처리 함수\n",
        "def preprocess_data(data, tokenizer, max_len):\n",
        "    # 기존 전처리 적용\n",
        "    data = preprocess_text(data)\n",
        "    # 문장 분리 적용\n",
        "    data = split_sentences(data)\n",
        "    # 토큰화 및 패딩\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th1WrkKSply3"
      },
      "source": [
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjmfptOT7ce5"
      },
      "outputs": [],
      "source": [
        "# ModelCheckpoint 설정 (최고 성능 모델만 저장)\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/My Drive/24-2Main/best_restaurant_sentiment_model.keras\",\n",
        "                             monitor='val_loss',\n",
        "                             save_best_only=True,\n",
        "                             verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrCd2AEokX8n"
      },
      "outputs": [],
      "source": [
        "# FastText 딕셔너리 불러오기\n",
        "with open(\"/content/drive/MyDrive/24-2Main/data/fasttext_embedding.pkl\", \"rb\") as f:\n",
        "    embedding_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "f-loM0sGJxfG",
        "outputId": "89c728e8-c7bb-4fd6-b046-5242bdf94faa"
      },
      "outputs": [],
      "source": [
        "# 2. 데이터 로드 및 초기 전처리\n",
        "from sklearn.utils import resample\n",
        "# 한국어 단발성 대화 데이터로 초기 학습\n",
        "labeled_data = pd.read_csv(\"/content/drive/MyDrive/24-2Main/data/한국어단발성데이터셋.csv\", index_col=False)  # 긍/부정 라벨 데이터\n",
        "labeled_data.rename(columns={\"Sentence\": \"reviews\", \"Emotion\":\"label\"}, inplace=True) # 함수 사용하기 위해서 칼럼명 맞춰드림\n",
        "\n",
        "# 클래스별 데이터 분리\n",
        "negative = labeled_data[labeled_data['label'] == 0]\n",
        "positive = labeled_data[labeled_data['label'] == 1]\n",
        "\n",
        "# 긍정 데이터 다운샘플링\n",
        "positive_downsampled = resample(positive,\n",
        "                                replace=False,  # 복원 샘플링 여부\n",
        "                                n_samples=4000,  # 4,000개로 샘플링\n",
        "                                random_state=42)\n",
        "\n",
        "# 부정 데이터 다운샘플링\n",
        "negative_downsampled = resample(negative,\n",
        "                                replace=False,  # 복원 샘플링 여부\n",
        "                                n_samples=4000,  # 4,000개로 샘플링\n",
        "                                random_state=42)\n",
        "\n",
        "# 다운샘플링 데이터 결합\n",
        "balanced_data = pd.concat([negative_downsampled, positive_downsampled])\n",
        "\n",
        "# 데이터 셔플링\n",
        "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "balanced_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8BcN9fMtJ0t_"
      },
      "outputs": [],
      "source": [
        "#토크나이저 정의 및 학습\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(balanced_data['reviews'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len = 80  # 리뷰 최대 길이\n",
        "\n",
        "with open(\"/content/drive/MyDrive/24-2Main/data/tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5WSb0AUjdtn",
        "outputId": "2bf8cb1f-26a8-448f-b1ce-b853a0509bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (24428, 300)\n"
          ]
        }
      ],
      "source": [
        "# 임베딩 행렬 생성\n",
        "embedding_dim = 300\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = create_embedding_matrix(tokenizer, embedding_dict, vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "_PB_WPDr0Llj",
        "outputId": "cf75bde3-af30-4cdd-ab4b-bca8e153818f"
      },
      "outputs": [],
      "source": [
        "print(\"1\", balanced_data)\n",
        "preprocessed_df = preprocess_data(balanced_data, tokenizer, max_len)\n",
        "print(\"2\", preprocessed_df)\n",
        "\n",
        "X = preprocessed_df['reviews']\n",
        "y = preprocessed_df['label']\n",
        "X.to_csv(\"/content/drive/MyDrive/24-2Main/data/X.csv\", index=False)\n",
        "y.to_csv(\"/content/drive/MyDrive/24-2Main/data/y.csv\", index=False)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(X if isinstance(X, list) else X)\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QCD1a9NAEY9"
      },
      "outputs": [],
      "source": [
        "# 4. 비정태적 CNN 모델 학습\n",
        "# 모델 빌드\n",
        "model = build_nonstatic_cnn(vocab_size, embedding_dim, embedding_matrix, max_len)\n",
        "\n",
        "# Early Stopping 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=10,\n",
        "          batch_size=64,\n",
        "          validation_data=(X_val, y_val),\n",
        "          callbacks=[early_stopping, checkpoint])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
