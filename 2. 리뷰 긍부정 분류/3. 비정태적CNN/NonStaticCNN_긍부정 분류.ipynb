{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3j8TIOzF_jT"
      },
      "source": [
        "# NonStaticCNN_긍부정 분류.ipynb\n",
        "- `NonStaticCNN_모델 만들기.ipynb`에서 만든 비정태적 CNN 모델으로 리뷰 데이터를 긍부정 분류 한다. \n",
        "- 긍정 - 1, 부정 - 0\n",
        "\n",
        "**필요한 파일** \n",
        "- `best_restaurant_semtiment_model.keras`\n",
        "- `tokenizer.pkl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SUZn7yiyF_jU",
        "outputId": "2c404392-b3bc-4dc4-be11-bc893eec4db6"
      },
      "outputs": [],
      "source": [
        "#'세션 다시 시작'이라는 안내메시지가 나오는 경우, \"취소\"하면 됨. (설치는 문제없이 되는듯??)\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
        "!pip install git+https://github.com/ssut/py-hanspell.git\n",
        "!pip install konlpy\n",
        "!pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i5VKi_plF_jU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import pickle\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from konlpy.tag import Kkma\n",
        "from pykospacing import Spacing\n",
        "from hanspell import spell_checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FBMVj4ciF_jV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Picked up JAVA_TOOL_OPTIONS: -Xmx4g -Xms512m\n"
          ]
        }
      ],
      "source": [
        "# 4. 데이터 전처리\n",
        "\n",
        "# 꼬꼬마 문장 분리 객체 생성\n",
        "kkma = Kkma()\n",
        "\n",
        "# 1. 전처리 함수: 특수문자 제거, 공백 처리, 맞춤법 교정\n",
        "def preprocess_text(data):\n",
        "    # 데이터프레임 형식이 아닌 경우 오류 처리\n",
        "    if isinstance(data, (list, np.ndarray)):\n",
        "        data = pd.DataFrame(data, columns=['reviews'])\n",
        "    elif not isinstance(data, pd.DataFrame):\n",
        "        raise ValueError(\"입력 데이터는 DataFrame, list, 또는 ndarray 형태여야 합니다.\")\n",
        "\n",
        "    # 한글, 공백, 언더스코어 외의 모든 문자 제거 (\\n 포함)\n",
        "    data['reviews'] = data['reviews'].str.replace(r\"[^\\sㄱ-ㅎㅏ-ㅣ가-힣]\", \" \", regex=True)\n",
        "\n",
        "    # 전처리 후 남은 글자가 2글자 미만이라면 삭제\n",
        "    data['reviews'] = data['reviews'].apply(lambda x: np.nan if isinstance(x, str) and len(x.strip()) <= 5 else x)\n",
        "    \n",
        "    #공백은 모두 nan으로 대체 \n",
        "    data['reviews'].replace('', np.nan, inplace=True)\n",
        "    \n",
        "    # NaN 제거\n",
        "    data.dropna(subset=['reviews'], inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 2. 문장 분리 함수: 꼬꼬마를 활용해 문장 단위로 분리\n",
        "def split_sentences(data):\n",
        "    if isinstance(data, list) or isinstance(data, np.ndarray):\n",
        "        data = pd.DataFrame(data, columns=['reviews'])\n",
        "\n",
        "    def safe_sentence_split(text):\n",
        "        try:\n",
        "            # 꼬꼬마로 문장 분리\n",
        "            if isinstance(text, str):\n",
        "                return kkma.sentences(text)\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            # 에러가 발생한 경우 원본을 로그로 남기고 리스트 형태로 반환\n",
        "            print(f\"문장 분리 중 오류 발생: {e}\\n문제의 데이터: {text}\")\n",
        "            return [text]\n",
        "\n",
        "    # 문장 분리 적용 (안전 처리)\n",
        "    data['reviews'] = data['reviews'].apply(safe_sentence_split)\n",
        "    data = data.explode('reviews', ignore_index=True)\n",
        "    return data\n",
        "\n",
        "# 3. 최종 데이터 전처리 함수\n",
        "def preprocess_data(data, tokenizer=None, max_len=None):\n",
        "    # 기존 전처리 적용\n",
        "    data = preprocess_text(data)\n",
        "    # 문장 분리 적용\n",
        "    data = split_sentences(data)\n",
        "    # 토큰화 및 패딩\n",
        "    if tokenizer and max_len:\n",
        "\n",
        "        data['reviews'] = data['reviews'].apply(lambda x: x if isinstance(x, str) else \"\")\n",
        "        # 텍스트를 정수 시퀀스로 변환\n",
        "        data['tokenized'] = tokenizer.texts_to_sequences(data['reviews'])\n",
        "        # 패딩 추가\n",
        "        data['tokenized'] = list(pad_sequences(data['tokenized'], maxlen=max_len, padding='post'))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qzUngfLoxmns"
      },
      "outputs": [],
      "source": [
        "with open(\"/Users/minjiku/Desktop/비정태적CNN/tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len=80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv-H6u-AQPzN"
      },
      "source": [
        "### 저장된 모델을 가져와서 라벨 찍기\n",
        "- 아래 과정을 모든 리뷰 데이터에 대해 수행하면 된다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v_/ccwz5s7931bf1s6yx_rzlzqm0000gn/T/ipykernel_51493/3337485792.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['reviews'].replace('', np.nan, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 분리 중 오류 발생: java.lang.OutOfMemoryError: Java heap space\n",
            "문제의 데이터: 마즈크진바그어어어어어어어어어어어어어어어어어어어어어어어어어어\n",
            "\u001b[1m2487/2487\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step\n",
            "예측 완료 및 저장 완료!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 저장된 모델 및 토크나이저 불러오기\n",
        "model = load_model(\"/Users/minjiku/Desktop/비정태적CNN/best_restaurant_sentiment_model.keras\")\n",
        "\n",
        "# 2. 데이터 불러오기\n",
        "unlabeled_data = pd.read_csv(\"/Users/minjiku/Desktop/비정태적CNN/maps.csv\")\n",
        "unlabeled_data = preprocess_data(unlabeled_data, tokenizer, max_len)\n",
        "unlabeled_data.to_csv(\"/Users/minjiku/Desktop/비정태적CNN/maps_processed.csv\", index=False)\n",
        "# unlabeled_data = pd.read_csv(\"/content/drive/MyDrive/24-2Main/data/youtube_processed.csv\", index_col=False)\n",
        "X_unlabeled = unlabeled_data['reviews']\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(X_unlabeled if isinstance(X_unlabeled, list) else X_unlabeled) #dataframe형태 보장\n",
        "X_unlabeled = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# 4. 전처리\n",
        "max_len = 80\n",
        "\n",
        "# 5. 모델 예측\n",
        "predictions = model.predict(X_unlabeled)\n",
        "\n",
        "# 6. 예측 결과 저장\n",
        "unlabeled_data['label'] = (predictions > 0.5).astype(int)  # 0: 부정, 1: 긍정\n",
        "\n",
        "# 7. 결과 저장\n",
        "unlabeled_data.to_csv(\"/Users/minjiku/Desktop/비정태적CNN/maps_labeled.csv\", index=False)\n",
        "\n",
        "print(\"예측 완료 및 저장 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 라벨을 바탕으로 식당별 긍부정 점수 계산\n",
        "- 부정-0, 긍정-1 이므로 식당별 점수 평균을 계산하면, 긍정 점수의 비율이 된다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#라벨이 포함된 데이터\n",
        "file_path = ''\n",
        "df = pd.read_csv(file_path)\n",
        "    \n",
        "#결과물 저장 경로\n",
        "output_path = ''\n",
        "df = df.groupby('restaurant')['label'].mean()\n",
        "df = df.to_frame()\n",
        "df = df.reset_index()\n",
        "df.to_csv(output_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
